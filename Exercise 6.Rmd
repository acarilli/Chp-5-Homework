---
title: "Exercise 6"
author: "Carilli"
date: '`r format(Sys.Date(), "%Y-%B-%d")`'
output: 
  html_document:
    theme: cerulean
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment = NA, warning = FALSE, message = FALSE,
                      options(digits = 3, scipen = 999))
require(tidyverse)
require(readr)
require(janitor)
require(broom)
require(magrittr)
require(kableExtra)
require(car)
require(moderndive)
library(lm.beta)
library(psycho)
```

```{r data}
globaled <- read_csv("globaled.csv") %>% 
  clean_names()
```

### (a)

```{r part_a}
globaled %$% 
  lm(ypcgr ~ testavg + edavg + ypc60) %>% 
  tidy()

globaled %$% 
  lm(ypcgr ~ testavg + edavg + ypc60) %>%   
  lm.beta()

lm(ypcgr ~ testavg + edavg + ypc60, globaled) %>% 
  psycho::standardize()
```

### (b)

We are testing:

$$\begin{aligned}
H_0&: \beta_1=\beta_2 \\
H_1&: \beta_1\ne\beta_2
\end{aligned}$$

The unrestricted model is

$$\text{UR:}\hspace{1cm}ypcgr=\beta_0+\beta_1testavg+\beta_2edavg+\beta_3ypc60+\epsilon$$

To form the restricted model let $\beta_1=\beta_2$.  This yields:

$$\text{R:}\hspace{1cm}ypcgr=\beta_0+\beta_1(testavg + edavg)+\beta_3ypc60+\nu$$

There are three ways we can test this hypothesis. 

1. Direct t-test where 

$$t\sim \frac{\left(\hat{\beta_1}-\hat{\beta_2}\right) - \left(\beta_1-\beta_2 \right) }{se_{\left(\hat{\beta_1}-\hat{\beta_2} \right)}}$$
where $$se_{\left(\hat{\beta_1}-\hat{\beta_2} \right)}=\sqrt{\text{Var}\left(\hat{\beta_1}\right) + \text{Var}\left(\hat{\beta_2}\right) - 2\times\text{Cov}\left(\hat{\beta_1},\hat{\beta_2}\right)}$$

```{r part_b_1}
# standardize the data (pyscho package) and estimate the equation
ols_s <- 
globaled %>% 
  mutate(ypcgr_s = standardize(ypcgr),
         testavg_s = standardize(testavg),
         edavg_s = standardize(edavg),
         ypc60_s = standardize(ypc60)) %$%
  lm(ypcgr_s ~ testavg_s + edavg_s + ypc60_s - 1)

ols_s %>% 
  tidy()

#retrieve the coefficients

beta <- 
ols_s %>% 
  tidy() %>% 
  pull(estimate)  

#generate the Var-Cov matrix 

var_cov <- vcov(ols_s)

df <- 
  ols_s %>% 
  glance() %>% 
    pull(df.residual)

(t <- (beta[1] - beta[2]) / sqrt(var_cov[1,1] + var_cov[2,2] - 2 * var_cov[1,2]))

qt(.975, df)

(1 - pt(t, df, lower.tail = TRUE))*2

```

2. Indirect t-test

Define a new parameter $\delta$ which takes on the value 0 under the null hypothesis. In this case $\delta=\beta_1-\beta_2$. Substitute into the unrestricted equation as $\beta_2=\delta-\beta_1$, estimate 

$$\begin{aligned}ypcgr&=\beta_0+\beta_1testavg + \left(\beta_1-\delta \right)edavg + \beta_3ypc60+\mu\\
&= \beta_0+\beta_1testavg + \beta_1 edavg - \delta edavg + \beta_3ypc60+\mu\\
&= \beta_0+\beta_1(testavg + edavg) - \delta edavg + \beta_3ypc60+\mu\\
\end{aligned}$$

and test $$H_0:\delta=0 \\ H_1:\delta\ne0$$
```{r part_b_2}
ols_idt <- 
  globaled %>% 
  mutate(ypcgr_s = standardize(ypcgr),
         testavg_s = standardize(testavg),
         edavg_s = standardize(edavg),
         ypc60_s = standardize(ypc60)) %$%
  lm(ypcgr_s ~ I(testavg_s + edavg_s) + edavg_s + ypc60_s - 1)

ols_idt %>% 
  tidy()
```


3. F-test (Wald test)

Where $$F \sim \frac{\frac{ess_R-ess_{UR}}{1}} {\frac{ess_{UR}}{n-k-1}}$$

```{r part_b_3}
ols_r <- 
globaled %>% 
  mutate(ypcgr_s = standardize(ypcgr),
         testavg_s = standardize(testavg),
         edavg_s = standardize(edavg),
         ypc60_s = standardize(ypc60)) %$%
  lm(ypcgr_s ~ I(testavg_s + edavg_s) + ypc60_s - 1)

ess_r <- 
  ols_r %>% 
  augment() %>% 
  summarize(ess = sum(.resid^2)) %>% 
  pull(ess)

ess_u <- 
  ols_s %>% 
    augment() %>% 
    summarize(ess = sum(.resid^2)) %>% 
    pull(ess)

(f_c <- (ess_r-ess_u)/(ess_u/df))

qf(.95, 1, df, lower.tail = TRUE)

pf(f_c, 1, df, lower.tail = FALSE)

```

We can perform this test by estimating each model and comparing the appropriate sum of squares or would could perform the test with `linearHhypothesis` from the `car` package as described in the companion.

```{r part_b}
ols_s %>% 
  linearHypothesis("testavg_s = edavg_s")
```

Note the p-values are the same an each case, the calculated t-stats are the same in parts 1 and 2.  Finally, note that the calculate F is the square of the calculate t. This is always the case when there is one degree of freedom in the numerator of an F-test.

### (c)

```{r part_c_i}
# we can also use scale from the base package to standardize the variables
globaled %>% 
  mutate(ypcgr_s = scale(ypcgr),
         testavg_s = scale(testavg),
         edavg_s = scale(edavg),
         ypc60_s = scale(ypc60),
         proprts_s = scale(proprts),
         open_s = scale(open)) %$%  
  lm(ypcgr_s ~ testavg_s + edavg_s + ypc60_s + proprts_s + open_s - 1) %>% 
  tidy()

```

```{r}
globaled %>% 
  mutate(ypcgr_s = scale(ypcgr),
         testavg_s = scale(testavg),
         edavg_s = scale(edavg),
         ypc60_s = scale(ypc60),
         proprts_s = scale(proprts),
         open_s = scale(open)) %$%  
  lm(ypcgr_s ~ testavg_s + edavg_s + ypc60_s + proprts_s + open_s - 1) %>% 
  linearHypothesis("testavg_s = proprts_s")
```



